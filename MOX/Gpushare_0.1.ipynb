{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from catalog\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "# Check if CUDA is available, if not, check for MPS (Metal Performance Shaders for Apple Silicon), otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load aligned_df from path\n",
    "file_path = \"aligned_df.pq\"\n",
    "data_set = pd.read_parquet(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyper-parameters \n",
    "\n",
    "num_classes = 1\n",
    "num_epochs = 5\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "\n",
    "# input_size = 5\n",
    "sequence_length = 100 # the window it trains with can be selected\n",
    "hidden_size = 32\n",
    "# hidden_size = 256 \n",
    "num_layers = 2\n",
    "step_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences for each experiment\n",
    "# sequence_length = 50  # Example sequence length\n",
    "\n",
    "# features = ['timestamp', 'A1_Sensor', 'A1_Sensor_diff', 'A1_Resistance', 'A1_Resistance_diff', 'A1_Sensor_norm', 'A1_Resistance_norm']\n",
    "features = ['timestamp_bin', 'A1_Resistance', 'A1_Resistance_diff', 'A1_Resistance_norm']\n",
    "target_column = 'resistance_ratio'\n",
    "\n",
    "input_size = len(features)  # Number of features\n",
    "\n",
    "# Initialize lists to hold sequences and targets\n",
    "sequences = []\n",
    "targets = []\n",
    "\n",
    "# ---------padding----------\n",
    "padding_length = 50  # Number of timesteps to pad at the end\n",
    "\n",
    "# Group by 'exp_no' and create sequences for each group\n",
    "for _, group in data_set.groupby('exp_no'):\n",
    "    # Only select the rows with the relevant columns\n",
    "    data = group[features].values\n",
    "    target_data = group[target_column].values\n",
    "    \n",
    "    # Pad the end of the dataset with zeros for features\n",
    "    pad_feature = np.zeros((padding_length, len(features)))\n",
    "    data = np.vstack((data, pad_feature))\n",
    "    \n",
    "    # Optionally, pad the end of the dataset with zeros or a specific value for targets\n",
    "    pad_target = np.zeros(padding_length)\n",
    "    target_data = np.concatenate((target_data, pad_target))\n",
    "\n",
    "    # Create sequences\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # Extract the sequence of features and the corresponding target\n",
    "        sequence = data[i:(i + sequence_length)]\n",
    "        target = target_data[i + sequence_length - 1]  # Target aligned with the end of the sequence\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        targets.append(target)\n",
    "# ---------padding----------\n",
    "\n",
    "# ---------no padding----------\n",
    "\n",
    "\n",
    "# # Group by 'exp_no' and create sequences for each group\n",
    "# for _, group in data_set.groupby('exp_no'):\n",
    "#     # Only select the rows with the relevant columns\n",
    "#     data = group[features].values\n",
    "#     target_data = group[target_column].values\n",
    "    \n",
    "#     # Create sequences\n",
    "#     for i in range(len(group) - sequence_length):\n",
    "#         # Extract the sequence of features and the corresponding target\n",
    "#         sequence = data[i:(i + sequence_length)]\n",
    "#         target = target_data[i + sequence_length]  # Target is the next record\n",
    "        \n",
    "#         sequences.append(sequence)\n",
    "#         targets.append(target)\n",
    "\n",
    "\n",
    "# ---------no padding----------\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "sequences_np = np.array(sequences)\n",
    "targets_np = np.array(targets)\n",
    "\n",
    "sequences_train, sequences_test, targets_train, targets_test = train_test_split(\n",
    "    sequences_np, targets_np, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Reshape, fit, and transform the training data\n",
    "n_samples_train, sequence_length, n_features = sequences_train.shape\n",
    "sequences_train_reshaped = sequences_train.reshape(-1, n_features)\n",
    "scaler.fit(sequences_train_reshaped)  # Fit only on training data\n",
    "sequences_train_scaled = scaler.transform(sequences_train_reshaped).reshape(n_samples_train, sequence_length, n_features)\n",
    "\n",
    "# Transform the testing data\n",
    "n_samples_test, _, _ = sequences_test.shape\n",
    "sequences_test_reshaped = sequences_test.reshape(-1, n_features)\n",
    "sequences_test_scaled = scaler.transform(sequences_test_reshaped).reshape(n_samples_test, sequence_length, n_features)\n",
    "\n",
    "# Convert numpy arrays to float32 before converting to PyTorch tensors\n",
    "sequences_np = sequences_np.astype(np.float32)\n",
    "targets_np = targets_np.astype(np.float32)\n",
    "\n",
    "train_sequences_tensor = torch.tensor(sequences_train_scaled, dtype=torch.float32)\n",
    "test_sequences_tensor = torch.tensor(sequences_test_scaled, dtype=torch.float32)\n",
    "\n",
    "train_targets_tensor = torch.tensor(targets_train, dtype=torch.float32)\n",
    "test_targets_tensor = torch.tensor(targets_test, dtype=torch.float32)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_sequences_tensor, train_targets_tensor)\n",
    "test_dataset = TensorDataset(test_sequences_tensor, test_targets_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "# batch_size = batch_size  # You can adjust this based on your memory constraints\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # -> x needs to be: (batch_size, seq, input_size)\n",
    "        \n",
    "        # or:\n",
    "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        \n",
    "        # x: (n, 28, 28), h0: (2, n, 128)\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        # out, _ = self.rnn(x, h0)  \n",
    "        # or:\n",
    "        out, _ = self.lstm(x, (h0,c0))  \n",
    "        \n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        # out: (n, 28, 128)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = out[:, -1, :]\n",
    "        # out: (n, 128)\n",
    "         \n",
    "        out = self.fc(out)\n",
    "        # out: (n, 10)\n",
    "        return out\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.0179478300479531, Validation RMSE: 0.018447727121241236\n",
      "Epoch 2, Training Loss: 0.0003660850780614467, Validation RMSE: 0.019212006961797222\n",
      "Epoch 3, Training Loss: 0.00034945835523202205, Validation RMSE: 0.019126504622256242\n",
      "Epoch 4, Training Loss: 0.00033930437102849104, Validation RMSE: 0.017670363111789853\n",
      "Epoch 5, Training Loss: 0.00033468627717593205, Validation RMSE: 0.017843368139127916\n"
     ]
    }
   ],
   "source": [
    "# Create a directory for checkpoints if it doesn't exist\n",
    "checkpoint_dir = './checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Assuming model, train_loader, and test_loader are defined as per your code\n",
    "# Assuming device selection logic is applied before this snippet\n",
    "model.to(device)  # Move your model to the GPU if available\n",
    "\n",
    "criterion = nn.MSELoss()  # For regression tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "training_losses = []\n",
    "validation_rmses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for sequences_batch, targets_batch in train_loader:\n",
    "        sequences_batch = sequences_batch.to(device)\n",
    "        targets_batch = targets_batch.to(device).unsqueeze(-1)\n",
    "        \n",
    "        outputs = model(sequences_batch)\n",
    "        loss = criterion(outputs, targets_batch)\n",
    "        epoch_losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_training_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    training_losses.append(avg_training_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for sequences_batch, targets_batch in test_loader:\n",
    "            sequences_batch = sequences_batch.to(device)\n",
    "            targets_batch = targets_batch.to(device).unsqueeze(-1)\n",
    "            \n",
    "            outputs = model(sequences_batch)\n",
    "            loss = criterion(outputs, targets_batch)\n",
    "            \n",
    "            total_val_loss += loss.item()\n",
    "            count += 1\n",
    "    \n",
    "    avg_val_loss = total_val_loss / count\n",
    "    val_rmse = math.sqrt(avg_val_loss)\n",
    "    validation_rmses.append(val_rmse)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Training Loss: {avg_training_loss}, Validation RMSE: {val_rmse}')\n",
    "    \n",
    "    # Save checkpoint\n",
    "    timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_{timestamp}.pt')\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'training_loss': avg_training_loss,\n",
    "        'validation_rmse': val_rmse,\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Optionally, save plots for loss and validation RMSE\n",
    "    if (epoch + 1) % 5 == 0:  # For example, save plots every 5 epochs\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, epoch + 2), training_losses, label='Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Over Time')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, epoch + 2), validation_rmses, label='Validation RMSE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('Validation RMSE Over Time')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(checkpoint_dir, f'plots_epoch_{epoch+1}_{timestamp}.png'))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m ig \u001b[38;5;241m=\u001b[39m IntegratedGradients(model)\n\u001b[1;32m     11\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m test_sequences_tensor[\u001b[38;5;241m10\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Add batch dimension, move to device, and ensure float32\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m attributions, delta \u001b[38;5;241m=\u001b[39m \u001b[43mig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m attributions \u001b[38;5;241m=\u001b[39m attributions\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Define the compute_importances function\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/kedro19/lib/python3.11/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/kedro19/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    274\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m _batch_attribution(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         num_examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[1;32m    296\u001b[0m     start_point, end_point \u001b[38;5;241m=\u001b[39m baselines, inputs\n",
      "File \u001b[0;32m~/.pyenv/versions/kedro19/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py:360\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    351\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_func(\n\u001b[1;32m    352\u001b[0m     forward_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_func,\n\u001b[1;32m    353\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mscaled_features_tpl,\n\u001b[1;32m    354\u001b[0m     target_ind\u001b[38;5;241m=\u001b[39mexpanded_target,\n\u001b[1;32m    355\u001b[0m     additional_forward_args\u001b[38;5;241m=\u001b[39minput_additional_args,\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m scaled_grads \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# aggregates across all steps for each tensor in the input tuple\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# total_grads has the same dimensionality as inputs\u001b[39;00m\n\u001b[1;32m    368\u001b[0m total_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    369\u001b[0m     _reshape_and_sum(\n\u001b[1;32m    370\u001b[0m         scaled_grad, n_steps, grad\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_steps, grad\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (scaled_grad, grad) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(scaled_grads, grads)\n\u001b[1;32m    373\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/kedro19/lib/python3.11/site-packages/captum/attr/_core/integrated_gradients.py:362\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    351\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_func(\n\u001b[1;32m    352\u001b[0m     forward_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_func,\n\u001b[1;32m    353\u001b[0m     inputs\u001b[38;5;241m=\u001b[39mscaled_features_tpl,\n\u001b[1;32m    354\u001b[0m     target_ind\u001b[38;5;241m=\u001b[39mexpanded_target,\n\u001b[1;32m    355\u001b[0m     additional_forward_args\u001b[38;5;241m=\u001b[39minput_additional_args,\n\u001b[1;32m    356\u001b[0m )\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[1;32m    360\u001b[0m scaled_grads \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     grad\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 362\u001b[0m     \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    364\u001b[0m ]\n\u001b[1;32m    366\u001b[0m \u001b[38;5;66;03m# aggregates across all steps for each tensor in the input tuple\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# total_grads has the same dimensionality as inputs\u001b[39;00m\n\u001b[1;32m    368\u001b[0m total_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    369\u001b[0m     _reshape_and_sum(\n\u001b[1;32m    370\u001b[0m         scaled_grad, n_steps, grad\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_steps, grad\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    371\u001b[0m     )\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (scaled_grad, grad) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(scaled_grads, grads)\n\u001b[1;32m    373\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "# Define the directory where you want to save your metrics\n",
    "base_directory = \"importance_results\"\n",
    "run_directory = os.path.join(base_directory, timestamp)\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(run_directory):\n",
    "    os.makedirs(run_directory)\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "input_tensor = test_sequences_tensor[10].unsqueeze(0).to(device).float()  # Add batch dimension, move to device, and ensure float32\n",
    "attributions, delta = ig.attribute(input_tensor, return_convergence_delta=True)\n",
    "attributions = attributions.float()\n",
    "\n",
    "# Define the compute_importances function\n",
    "def compute_importances(model, input_sequence):\n",
    "    # Initialize IntegratedGradients with the model\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    # Ensure the input sequence tensor is in float32 and add a batch dimension\n",
    "    input_tensor = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Compute attributions using Integrated Gradients\n",
    "    attributions, delta = ig.attribute(input_tensor, return_convergence_delta=True)\n",
    "    \n",
    "    # Ensure the returned attributions are in float32\n",
    "    attributions = attributions.float()\n",
    "    \n",
    "    # Return the attributions and delta as numpy arrays\n",
    "    return attributions.detach().cpu().numpy(), delta.detach().cpu().numpy()\n",
    "# Assuming 'model', 'device', and 'compute_importances' function are defined as before\n",
    "\n",
    "# Select only the group for exp_no = 0\n",
    "group = data_set[data_set['exp_no'] == 0]\n",
    "\n",
    "total_timesteps = len(group)  # Total timesteps for exp_no = 0\n",
    "number_of_features = len(features)  # Assuming 'features' is a list of feature names\n",
    "sequence_length = 100  # Assuming sequence length is defined\n",
    "\n",
    "# Initialize arrays for cumulative importances and contributions\n",
    "cumulative_importances = np.zeros((total_timesteps, number_of_features))\n",
    "contributions = np.zeros(total_timesteps)\n",
    "\n",
    "all_importances = []  # List to store all importances for sequences\n",
    "for i in range(total_timesteps - sequence_length + 1):\n",
    "    sequence = group[features].values[i:(i + sequence_length)]\n",
    "    # Compute importances for the sequence\n",
    "    importances, _ = compute_importances(model, sequence)\n",
    "    all_importances.append(importances)\n",
    "\n",
    "# Aggregate the importances in a temporary array first\n",
    "for idx, imp in enumerate(all_importances):\n",
    "    start_idx = idx\n",
    "    end_idx = start_idx + sequence_length\n",
    "\n",
    "    imp_reshaped = imp.squeeze(0)\n",
    "\n",
    "    # Directly update the cumulative importances and contributions\n",
    "    cumulative_importances[start_idx:end_idx, :] += imp_reshaped\n",
    "    contributions[start_idx:end_idx] += 1\n",
    "\n",
    "# Calculate average importances by avoiding division by zero\n",
    "avg_importances = np.zeros_like(cumulative_importances)\n",
    "non_zero_contributions = contributions > 0\n",
    "avg_importances[non_zero_contributions, :] = cumulative_importances[non_zero_contributions, :] / contributions[non_zero_contributions, None]\n",
    "\n",
    "# Plot the average importances for exp_no = 0\n",
    "timesteps = np.arange(total_timesteps)  # An array of timesteps from 0 to the last one\n",
    "\n",
    "for feature_idx in range(number_of_features):\n",
    "    plt.plot(timesteps, avg_importances[:, feature_idx], label=f'Feature {feature_idx + 1}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('Average Feature Importance')\n",
    "plt.title('Average Feature Importances Over Time for Exp 0')\n",
    "\n",
    "# Save the plot in the run-specific directory with a meaningful name\n",
    "plt.savefig(os.path.join(run_directory, 'average_feature_importances.png'))\n",
    "plt.close()  # Close the plot to avoid displaying it inline if not needed\n",
    "\n",
    "# If you need to save the numeric data as well:\n",
    "np.save(os.path.join(run_directory, 'avg_importances.npy'), avg_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the compute_importances function\n",
    "def compute_importances(model, input_sequence):\n",
    "    # Initialize IntegratedGradients with the model\n",
    "    ig = IntegratedGradients(model)\n",
    "    \n",
    "    # Ensure the input sequence tensor is in float32 and add a batch dimension\n",
    "    input_tensor = torch.tensor(input_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Compute attributions using Integrated Gradients\n",
    "    attributions, delta = ig.attribute(input_tensor, return_convergence_delta=True)\n",
    "    \n",
    "    # Ensure the returned attributions are in float32\n",
    "    attributions = attributions.float()\n",
    "    \n",
    "    # Return the attributions and delta as numpy arrays\n",
    "    return attributions.detach().numpy(), delta.detach().numpy()\n",
    "\n",
    "# Dictionary to store aggregated importances for each experiment\n",
    "experiment_importances = {}\n",
    "\n",
    "for exp_no, group in data_set.groupby('exp_no'):\n",
    "    # Initialize a list to store importances for all sequences in this experiment\n",
    "    all_importances = []\n",
    "    \n",
    "    # Compute importances for each sequence within this experiment\n",
    "    for i in range(len(group) - sequence_length):\n",
    "        sequence = group[features].values[i:(i + sequence_length)]\n",
    "        # Assuming sequence is in the correct shape for the model\n",
    "        importances = compute_importances(model, sequence)\n",
    "        all_importances.append(importances)\n",
    "    \n",
    "    # Aggregate the importances across all sequences for this experiment\n",
    "    # Here, we're taking the mean, but you could also sum them or use another method\n",
    "    aggregated_importances = np.mean(all_importances, axis=0)\n",
    "    \n",
    "    # Store the aggregated importances in the dictionary\n",
    "    experiment_importances[exp_no] = aggregated_importances\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kedro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
